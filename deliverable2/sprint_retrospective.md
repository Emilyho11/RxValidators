# Successes:
- Getting web scrapers to work using Scrapy
- Importing data into Pandas dataframes and testing the scrapers with it
- Copying the cURL and testing in Postman
- Employed SOLID principles such as Single Responsibility
- Managed to use pymongo to connect to the database
- Managed to create the PDF in the format the company wants (including their unique prescriber codes)

# Challenges:
- Finding time to work on this project  
- Designing the functions’ input and output (2D array or dictionary)
- Deciding on the database to use based on our needs (1 or 2 databases)
- Figuring out and understanding how to web scrape
- Setting up prerequisites for each of our user stories
- Figuring out how to implement our tasks in the most efficient way possible
  - Selenium, Scrapy, etc.
  - How to match the licence if two physicians have the same name
  - Parameters (arrays or dictionaries)
  - What specific tasks our functions will perform
- Figuring out a time to meet
- Integrating all the web scrapers together

# To Improve for the Next Sprint:
- Our pacing (story points per person per sprint, assigning each member with more tasks, making the work more equally spread out)
- Better communication among team members
- Those working on similar tasks, give frequent updates to each other so that code and implementation are consistent
  - Start integration earlier in the sprint
- Agreeing on what we will use (data types,...) before implementing our tasks
- Push our code more often to GitHub so that team members can see each other’s progress
- Use Google Docs instead of a .md file when logging our standup meetings
